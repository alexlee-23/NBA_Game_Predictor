{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('NBA_temporal_dataset.pkl', 'rb') as file:\n",
    "    temporal_data_dict = pickle.load(file)\n",
    "\n",
    "X_primary = temporal_data_dict['X_primary']\n",
    "X_opposing = temporal_data_dict['X_opposing'] \n",
    "y = temporal_data_dict['y']\n",
    "\n",
    "print(X_primary.shape)\n",
    "print(X_opposing.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_primary_torch = torch.from_numpy(X_primary).float()\n",
    "X_opposing_torch = torch.from_numpy(X_opposing).float()\n",
    "y_torch = torch.from_numpy(y).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NBADataset(Dataset):\n",
    "    def __init__(self, X_primary_data, X_opposing_data, y_data):\n",
    "        self.X_primary_data = X_primary_data\n",
    "        self.X_opposing_data = X_opposing_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_primary_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_primary_data[idx], self.X_opposing_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = NBADataset(X_primary_torch, X_opposing_torch, y_torch)\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if loader is loading properly\n",
    "\n",
    "for x_primary, x_opposing, y in train_loader:\n",
    "    print(\"Batch X_primary:\", x_primary.shape)  # should be [batch_size, 5, 42]\n",
    "    print(\"Batch X_opposing:\", x_opposing.shape) # should be [batch_size, 5, 42]\n",
    "    print(\"Batch y:\", y.shape)  # should be [batch_size, 1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dual LSTM module ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiameseDualLSTM(nn.Module):\n",
    "    def __init__(self, input_size=42, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super(SiameseDualLSTM, self).__init__()\n",
    "        \n",
    "        # LSTM for primary team\n",
    "        self.lstm_primary = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # LSTM for opposing team\n",
    "        self.lstm_opposing = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers after concatenating both LSTM outputs\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, primary_input, opposing_input):\n",
    "        # Primary team LSTM\n",
    "        _, (primary_out, _) = self.lstm_primary(primary_input)\n",
    "        \n",
    "        # Opposing team LSTM\n",
    "        _, (opposing_out, _) = self.lstm_opposing(opposing_input)\n",
    "        \n",
    "        # Concatenate the final hidden states from both LSTMs (batch_size, hidden_size*2)\n",
    "        combined = torch.cat((primary_out[-1], opposing_out[-1]), dim=1)\n",
    "        \n",
    "        # Feed through fully connected layers\n",
    "        x = self.fc1(combined)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        #Using BCE with Logits, no need for sigmoid\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and test model \n",
    "import time\n",
    "\n",
    "\n",
    "def AccuracyAndLossOverEpoch(model, train_loader, test_loader, optimizer, criterion, device, epochs):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    time_to_train_per_epoch_list = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Set current epoch train loss, train accuracy and test accuracy to 0.0\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        test_acc = 0.0\n",
    "        time_per_epoch = 0.0\n",
    "        # Put model in training mode\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for X_primary_batch, X_opposing_batch, y_batch in train_loader:\n",
    "            X_primary_batch = X_primary_batch.to(device)\n",
    "            X_opposing_batch = X_opposing_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_primary_batch, X_opposing_batch)\n",
    "            # Get loss on current batch\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate train loss in epoch\n",
    "            train_loss += loss.item()\n",
    "        # Add train loss to train_losses, in order to track\n",
    "        # print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}\")\n",
    "        train_losses.append(train_loss)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate time per epoch and accumulate\n",
    "        time_per_epoch = end_time - start_time\n",
    "        time_to_train_per_epoch_list.append(time_per_epoch)\n",
    "        \n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        with torch.no_grad():\n",
    "            # Iterate over train dataset, in order to get accuracy for model at current epoch\n",
    "            for X_primary_batch, X_opposing_batch, y_batch in train_loader:\n",
    "                X_primary_batch = X_primary_batch.to(device)\n",
    "                X_opposing_batch = X_opposing_batch.to(device)\n",
    "\n",
    "                y_pred = model(X_primary_batch, X_opposing_batch)\n",
    "                y_pred = nn.Sigmoid()(y_pred)\n",
    "                y_pred = (y_pred >= 0.5).float().round()\n",
    "                correct += (y_pred == y_batch).sum().item()\n",
    "                total += len(y_batch)\n",
    "        # Add train accuracy to train_accuracies\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Training Accuracy: {train_acc}\")\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        with torch.no_grad():\n",
    "            # Iterate over test dataset, in order to get accuracy for model at current epoch\n",
    "            for X_primary_batch, X_opposing_batch, y_batch in test_loader:\n",
    "                X_primary_batch = X_primary_batch.to(device)\n",
    "                X_opposing_batch = X_opposing_batch.to(device)\n",
    "\n",
    "                y_pred = model(X_primary_batch, X_opposing_batch)\n",
    "                y_pred = nn.Sigmoid()(y_pred)\n",
    "                y_pred = (y_pred >= 0.5).float()\n",
    "                \n",
    "                correct += (y_pred == y_batch).sum().item()\n",
    "                total += len(y_batch)\n",
    "        # Add test accuracy to test_accuracies\n",
    "        test_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Test Accuracy: {test_acc}\")\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies, time_to_train_per_epoch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, optimizer and loss function\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseDualLSTM(input_size=42, hidden_size=128, num_layers=2, dropout=0.3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and test model \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_losses, train_accuracies, test_accuracies, time_to_train_per_epoch_list \u001b[38;5;241m=\u001b[39m \u001b[43mAccuracyAndLossOverEpoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m, in \u001b[0;36mAccuracyAndLossOverEpoch\u001b[1;34m(model, train_loader, test_loader, optimizer, criterion, device, epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get loss on current batch\u001b[39;00m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Accumulate train loss in epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex1\\.conda\\envs\\cs5100\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex1\\.conda\\envs\\cs5100\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex1\\.conda\\envs\\cs5100\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and test model \n",
    "train_losses, train_accuracies, test_accuracies, time_to_train_per_epoch_list = AccuracyAndLossOverEpoch(model, train_loader, test_loader, optimizer, criterion, device, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_loss_train_test_accuracies(train_accuracies, test_accuracies, time_to_train_per_epoch_list, model_name):\n",
    "\n",
    "    print(f\"Total time to train model: {sum(time_to_train_per_epoch_list)}\")\n",
    "\n",
    "    print(\"Number of epochs: \", len(time_to_train_per_epoch_list))\n",
    "\n",
    "    print(\"Average time per epoch: \", sum(time_to_train_per_epoch_list) / len(time_to_train_per_epoch_list))\n",
    "\n",
    "    epochs = list(range(1, len(time_to_train_per_epoch_list) + 1))  # Epoch numbers\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(epochs, time_to_train_per_epoch_list, color='blue', alpha=0.7)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"{model_name} - Training Time per Epoch\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(model_name + \" Loss over Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "    plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(model_name + \" Accuracy over Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "plot_time_loss_train_test_accuracies(train_accuracies, test_accuracies, time_to_train_per_epoch_list, \"Dual LSTM Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluating model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_ffn_classifier(model, test_dataset, model_name=\"FFN\", batch_size=512):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_primary_batch, X_opposing_batch, y_batch in test_loader:\n",
    "            X_primary_batch = X_primary_batch.to(device)\n",
    "            X_opposing_batch = X_opposing_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_primary_batch, X_opposing_batch).squeeze()\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend((probs >= 0.5).int().cpu().numpy())\n",
    "            all_labels.extend(y_batch.int().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nEvaluation for {model_name}\")\n",
    "    print(f\"Accuracy     : {acc:.4f}\")\n",
    "    print(f\"Precision    : {prec:.4f}\")\n",
    "    print(f\"Recall       : {rec:.4f}\")\n",
    "    print(f\"F1 Score     : {f1:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_distribution_ffn(model, test_dataset, model_name=\"FFN\", batch_size=512):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_primary_batch, X_opposing_batch, y_batch in test_loader:\n",
    "            X_primary_batch = X_primary_batch.to(device)\n",
    "            X_opposing_batch = X_opposing_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_primary_batch, X_opposing_batch).squeeze()\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels).ravel()\n",
    "\n",
    "    true_win_probs = all_probs[all_labels == 1]\n",
    "    true_loss_probs = all_probs[all_labels == 0]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(true_win_probs, color=\"green\", label=\"True Wins\", kde=True, stat=\"density\", bins=30)\n",
    "    sns.histplot(true_loss_probs, color=\"red\", label=\"True Losses\", kde=True, stat=\"density\", bins=30)\n",
    "\n",
    "    plt.title(f\"{model_name} – Probability Distribution of Predicted Wins\")\n",
    "    plt.xlabel(\"Predicted Probability of Win\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve_ffn(model, test_dataset, model_name=\"FFN\", batch_size=512):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_primary_batch, X_opposing_batch, y_batch in test_loader:\n",
    "            X_primary_batch = X_primary_batch.to(device)\n",
    "            X_opposing_batch = X_opposing_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_primary_batch, X_opposing_batch).squeeze()\n",
    "            \n",
    "            probs = torch.sigmoid(logits)  # convert logits to probabilities\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve – {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_ffn_classifier(model, test_dataset, model_name=\"Dual LSTM\")\n",
    "plot_prediction_distribution_ffn(model, test_dataset, model_name=\"Dual LSTM\")\n",
    "plot_roc_curve_ffn(model, test_dataset, model_name=\"Dual LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "path = os.path.join('.', 'models', 'dual_lstm_model.pth')\n",
    "\n",
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the model's state_dict\n",
    "\n",
    "model = SiameseDualLSTM(input_size=42, hidden_size=128, num_layers=2, dropout=0.3)\n",
    "path = os.path.join('.', 'models', 'dual_lstm_model.pth')\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
